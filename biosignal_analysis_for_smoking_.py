# -*- coding: utf-8 -*-
"""Biosignal analysis for smoking .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W_ffVfCIPn0QBhJ0J11jFjYdONZP8Kiw
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/smoking.csv')

df

df.shape

df.info(
)

df = df.drop(columns=['ID' , 'oral'])

df.isnull().sum()

df.duplicated().sum()

df[df.duplicated(keep=False)].head(10)
#no need to drop duplicates but data is not identical

sns.barplot(data=df, x='gender', y='smoking')

sns.countplot(data=df, x='gender', hue='smoking')

plt.figure(figsize=(10, 8))
df['smoking'].value_counts().plot.pie(autopct = '%1.1f%%')
plt.title('smoking')

for i in df.columns:
  if(df[i].dtypes == 'int64' or df[i].dtypes == 'float64'):
    sns.boxplot(df[i])
    plt.show()
    '''Representation of columns using boxplot to detect outliers. Here outliers represent natural
variations in the population, and they should be left as is in the dataset. These are called true
outliers. Therefore for this dataset we will not remove outliers.'''

for i in df.columns:
  if(df[i].dtypes == 'int64' or df[i].dtypes == 'float64'):
    sns.histplot(df[i])
    plt.show()

df.info()

df['dental caries'].sample(10)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for col in ['gender', 'tartar']:
  df[col] = le.fit_transform(df[col])

df.info(

)

"""# **Feature Selection **

```
Feature importance is a technique
that calculate a score for all the
input features for a given model. So
out of 24 features we will select the
top 15 features based on the score.
```
very very very imp


"""

X = df.iloc[: , :-1]
Y = df['smoking']
from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
model.fit(X,Y)
df1 = pd.Series(model.feature_importances_, index=X.columns)
df1.nlargest(15).plot(kind='barh')
plt.show()

X = df[['AST' , 'relaxation' , 'LDL' , 'systolic' , 'fasting blood sugar' , 'ALT' , 'HDL' , 'waist(cm)' , 'age' , 'weight(kg)' , 'age' ,
        'hemoglobin' , 'triglyceride' , 'height(cm)' , 'Gtp' , 'gender']]

Y = df['smoking']
from sklearn.model_selection import train_test_split
X_train , X_test , Y_train , Y_test = train_test_split(X , Y , test_size=0.2 , random_state=42)
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler



Y = df['smoking']
from sklearn.model_selection import train_test_split
X_train , X_test , Y_train , Y_test = train_test_split(X , Y , test_size=0.2 , random_state=42)
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
lr = LogisticRegression()
lr.fit(X_train , Y_train)
Y_pred = lr.predict(X_test)
from sklearn.metrics import accuracy_score , classification_report
accuracy_score(Y_test , Y_pred)
classification_report(Y_test , Y_pred)

Y = df['smoking']
from sklearn.model_selection import train_test_split
X_train , X_test , Y_train , Y_test = train_test_split(X , Y , test_size=0.2 , random_state=42)
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
lr = LogisticRegression()
lr.fit(X_train , Y_train)
Y_pred = lr.predict(X_test)
from sklearn.metrics import accuracy_score , classification_report
accuracy_score(Y_test , Y_pred)

classification_report(Y_test , Y_pred)

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train , Y_train)
Y_pred = dt.predict(X_test)
accuracy_score(Y_test , Y_pred)

"""# ***Bagging Algorithm ***

---
Bootstrap Aggregation or bagging involves taking multiple samples from the training dataset
(with replacement) and training a model for each sample.
---



"""

from sklearn.ensemble import BaggingClassifier
bc = BaggingClassifier()
bc.fit(X_train , Y_train)
Y_pred = bc.predict(X_test)
classification_report(Y_test , Y_pred)

from sklearn.ensemble import BaggingClassifier
bagging_clf = BaggingClassifier(estimator = DecisionTreeClassifier() , n_estimators=1000 )
bagging_clf.fit(X_train, Y_train)
Y_pred = bagging_clf.predict(X_test)
accuracy_score(Y_test , Y_pred)

from sklearn.ensemble import ExtraTreeeClassifier
et = ExtraTreeeClassifier(n_estimator = 1000 , random_state = 45)
et.fit(X_train , Y_train)
Y_pred = et.predict(X_test)
accuracy_score(Y_test , Y_pred)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 1000)
rf.fit(X_train , Y_train)
Y_pred = rf.predict(X_test)
accuracy_score(Y_test , Y_pred)